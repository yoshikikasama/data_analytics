# SNOWPRO® ASSOCIATE: PLATFORM EXAM 対策ガイド

## 試験概要

### 認定内容
この試験では、以下の知識とスキルを検証します：
- SnowflakeユーザーインターフェースとSnowflake Notebooksのセットアップと操作
- データベース、ステージ、コンピュートリソースの作成
- 構造化、半構造化、非構造化データの読み込みと活用
- Snowflakeロールとデータアクセス管理の理解
- Snowflakeアカウント構造の理解と管理
- Snowflake Cortex LLM関数の呼び出し

### 試験範囲と配点
- **1.0 Interacting with Snowflake and the Architecture: 35%**
- **2.0 Identity and Data Access Management: 15%**
- **3.0 Data Loading and Virtual Warehouses: 40%**
- **4.0 Data Protection and Data Sharing: 10%**

---

## 1.0 Interacting with Snowflake and the Architecture (35%)

### 1.1 Snowflake AI Data Cloudの主要機能とメリット

#### Elastic storage（弾性ストレージ）
必要に応じて自動的にスケールするストレージシステム

#### Elastic compute（弾性コンピュート）
ワークロードに応じてリソースを調整可能なコンピュート環境

#### Snowflake layers（Snowflakeの層構造）

**クラウドサービス層（Cloud Services Layer）**
- データの管理やセキュリティを担う層
- ユーザーがSnowflakeを利用する際の認証、アクセス制御、メタデータ管理、クエリの最適化などを担当
- 主な役割：
  - 認証・アクセス管理 → 誰がどのデータにアクセスできるかを制御
  - クエリの最適化 → データ検索を効率化し、高速に処理
  - メタデータ管理 → データの構造や変更履歴を管理

**コンピュート層（Compute Layer / クエリ処理層）**
- データ処理を実行する層
- Snowflakeでは「仮想ウェアハウス」と呼ばれる仕組みを使い、ウェアハウスごとに独立したコンピュートリソースを提供
- ウェアハウスを分けることにより、ETLでのロード処理とBIツールの参照処理など異なるワークロードの処理が依存することがなくなる
- 例えば、大量データのロードでETL処理がリソースを大量に使った場合もBIツールの参照処理の性能に全く影響することは無い
- 主な役割：
  - データの検索・集計・計算を実行（SQLクエリを処理）
  - 負荷に応じて処理能力を増減させ、スケーラブルな処理を実行
  - 複数のクエリを並行実行し、高速な処理を実現

**ストレージ層（Storage Layer）**
- データを保存する層
- Snowflakeではデータをクラウドストレージ上に列指向フォーマットで格納し、効率的に管理・圧縮
- 主な役割：
  - データの管理と保存
  - 列指向フォーマットによる高速なデータ取得（分析に最適化）
  - マイクロパーティショニングによるデータ分割と最適化

#### アーキテクチャの比較

**従来のアーキテクチャ**
- 共有ストレージシングルクラスタ (Shared Everything)
- 分散型のローカルストレージシングルクラスタ (Shared Nothing)
- 複数のワークロードを互いに影響を与えずに実行することが難しいアーキテクチャ

**Snowflakeのアーキテクチャ**
- Multi Cluster, Shared Data
- 集中化されたスケールアウト可能なクラウドストレージ
- 複数の独立したコンピュートクラスタ
- データを一箇所に管理し、複数のワークロードをWLMなしで実行できるアーキテクチャ

### 1.2 主要なSnowflakeユーザーインターフェース

#### Snowsight
Snowflakeのメインウェブインターフェース

#### Snowflake Notebooks
インタラクティブな開発環境でSQLとPythonの両方をサポート

#### Worksheets
クエリ実行環境として以下をサポート：
- Python
- SQL

### 1.3 Snowsightユーザーインターフェースの使用方法

#### データ読み込み（Data loading）
GUIを通じた直感的なデータインポート機能

#### クエリ履歴（Query history）
過去のクエリ実行履歴の確認と再実行が可能

#### オブジェクトブラウザ（Object browsers）
以下のオブジェクトを作成可能：
- データベース（databases）
- スキーマ（schemas）
- ステージ（stages）

### 1.4 Snowflake Notebooksでの作業

#### ノートブックセッション（Notebook sessions）
対話的な開発環境として機能

#### コードの実行（Run code）
以下の言語でコード実行が可能：
- SQL
- Python

#### セル実行ステータス（Cell execution status）
実行状況の可視化機能

#### Streamlitを使用したデータ可視化（Visualize data using Streamlit）
インタラクティブなダッシュボードとアプリケーションの作成

#### Python変数置換（Python variable substitution）
PythonとSQLの動的な連携機能

### 1.5 Snowflakeオブジェクトとオブジェクト階層

#### 主要オブジェクト
- **データベース（Databases）**: 最上位のデータコンテナ
- **スキーマ（Schemas）**: データベース内の論理的な分割
- **テーブル（Tables）**: 実際のデータを格納するオブジェクト
- **ビュー（Views）**: 仮想的なテーブル
- **データ型（Data types）**: 各種データ形式のサポート

#### オブジェクト階層構造

**Organization Level（組織レベル）**
- Organization（組織）

**Account Level（アカウントレベル）** - contained in Organization
- Account（アカウント）
- User（ユーザー）
- Role（ロール）
- Warehouse（ウェアハウス）
- Resource Monitor（リソースモニター）
- Network Policy（ネットワークポリシー）
- Share（共有）
- Reader Account（リーダーアカウント）

**Database Level（データベースレベル）** - contained in Account
- Database（データベース）

**Schema Level（スキーマレベル）** - contained in Database
- Schema（スキーマ）
- Stage（ステージ）
- File Format（ファイルフォーマット）
- Pipe（パイプ）
- Stream（ストリーム）
- Task（タスク）
- Function（関数）
- Procedure（プロシージャ）
- Sequence（シーケンス）

**Table and Object Level（テーブルとオブジェクトレベル）** - contained in Schema
- Table（テーブル）
- View（ビュー）
- Materialized View（マテリアライズドビュー）
- External Table（外部テーブル）

---

## 2.0 Identity and Data Access Management (15%)

### 2.1 Snowflakeで使用されるロール

#### Role-Based Access Control (RBAC)
**ロール階層の説明（Explain role hierarchy）**
親ロールは子ロールの権限を継承する仕組み

**ロールタイプ（Role types）**
システムロールとカスタムロールに分類

**権限（Privileges）**
オブジェクトに対するアクセス制御の定義

**ロールによるオブジェクトアクセス（Object access by role）**
権限の付与と管理システム

#### システムロール

**ACCOUNTADMIN**
- 全権限を持つ（SECURITYADMINとSYSADMINの親ロール）

**SECURITYADMIN**
- ユーザーやロールに加え、各種権限の管理を行う（USERADMINの親ロール）
- MANAGE GRANTS権限を持つ

**USERADMIN**
- ユーザーやロールの管理を行う

**SYSADMIN**
- 仮想ウェアハウスやDBなどのオブジェクトの管理を行う

**PUBLIC**
- 全ユーザーのデフォルトロール
- 権限なし（あとから付与可能）

#### カスタムロール
- システムロールとは別にユーザー側で任意のロールを作成できる
- 一般的にはUSERADMIN（SECURITYADMIN）が作成を行う
- 全てのカスタムロールをSYSADMINの子ロールにする
  - 各種データ関係のオブジェクトの管理をSYSADMINであれば見れるように
- 逆にそれ以外でロール作成を行うとロールのガバナンスが取れなくなる

#### Snowflakeの推奨するRBACの設計方法
**Functional Role（役割ロール）とAccess Role（アクセスロール）を組み合わせたロール構成**

**Functional Role**
- 部門や職位など、組織のビジネス機能に対応するロール
- Access Roleを継承し、ユーザーにGRANTするロール

**Access Role**
- Snowflakeオブジェクトへのアクセス権だけを付与したロール
- テーブルであれば、Read, Write, Insert, DeleteなどのPrivilageを制御

### 2.2 データベースの作成、データ探索、パラメータ設定、所有権譲渡

#### データベースオブジェクトの使用（Use database objects）

**INFORMATION_SCHEMAの使用（Use the INFORMATION_SCHEMA）**
システムメタデータへのアクセス機能

**PUBLIC SCHEMAの使用（Use the PUBLIC SCHEMA）**
デフォルトスキーマの活用

#### コンテキストの理解（Understand context）
現在のデータベース・スキーマ・ロールの把握

#### 所有権譲渡（Transfer ownership）
オブジェクト所有者の変更機能

#### スキーマの作成と削除（Create and drop schemas）
論理的なデータ分割の管理

#### 基本SQLコマンドの実行（Run basic SQL commands）
- **EXCLUDE**: 特定の列を除外
- **SELECT ***: 全列の選択
- **LIMIT**: 結果行数の制限

#### オブジェクトの所有権について
- 全てのオブジェクトは単一のロールが所有している
- デフォルトではオブジェクトを作成したロールがそのまま所有者になる
- オブジェクトの所有者であるロールだけが、権限の付与や取り消し、所有権の譲渡などを行うことができる

#### Container（コンテナ）とObject（オブジェクト）の概念

**Container（コンテナ）**
- コンテナはオブジェクトを保持し、整理する
- コンテナはオブジェクトを論理的にグループ化するために使用され、アクセス管理とデータの効率的な整理に役立つ
- 例えば、Snowflakeのデータベースは、複数のスキーマとオブジェクトを保持するコンテナー

**オブジェクト**

*アカウントオブジェクト:*
- ユーザー（User）
- ロール（Role）
- ウェアハウス（Warehouse）
- リソースモニター（Resource Monitor）
- ネットワークポリシー（Network Policy）

*データオブジェクト:*
- Snowflake（および一般的なデータベース）では、「オブジェクト」はデータを保存または操作できる個々のエンティティを指す
- 例としては、テーブル（データが格納される場所）、ビュー（データのクエリ方法）、スキーマ（データをカテゴリに整理する）などがある
- オブジェクトは、Snowflakeにおける基本的な構成要素

---

## 3.0 Data Loading and Virtual Warehouses (40%)

### 3.1 構造化データと半構造化データを扱う際の考慮事項

#### ステージ（Stages）
データロードの準備場所として機能

**stage（ステージ）の概念**
- 一時置き場
- ロードするファイルを保持するクラウドファイルレポジトリ
- 内部または外部
  - 内部: Snowflakeが管理するストレージ内
  - 外部: クラウドプロバイダーが管理するクラウドストレージ
- データを最初にロードすることなく直接クエリできる

**ステージの種類**

*外部ステージ（External Stage）*
- Snowflake外のクラウドストレージ（S3、GCS、Azure Blobなど）を指定
- `CREATE STAGE`で外部URL・認証情報を明示的に指定
- 外部サービス連携や既存ファイル資産の活用に最適

*内部ステージ（Internal Stage）*
- Snowflakeが管理するストレージ（Snowflakeアカウント内）
- 暗号化：`SNOWFLAKE_SSE`または`SNOWFLAKE_FULL`を選択可能
- さらに以下の3つに分類される：

  **ユーザーステージ（User Stage）**
  - 各ユーザーに自動的に割り当てられるステージ
  - 一時ファイルや開発中のデータ保管に便利
  - アクセス：`@~`

  **テーブルステージ（Table Stage）**
  - 各テーブルごとに自動的に付与されるステージ
  - テーブルへの`COPY INTO`や`EXPORT`時に使用
  - アクセス：`@%table_name`

  **名前付きステージ（Named Stage）**
  - `CREATE STAGE`により明示的に作成されるステージ
  - チーム内での共有、外部接続、カタログ連携などに活用
  - DIRECTORYオプションや暗号化、セキュリティ設定可能
  - アクセス：`@stage_name`

- ステージにはファイルをアップロード（PUT）し、SQLやAPIで利用可能

#### データ読み込み（Loading data）

**構造化データ（Structured data）**
- CSV、固定幅ファイルなど
- 明確なスキーマ定義が必要

**半構造化データ（Semi-structured data）**
- 固定スキーマを持たないが、タグやマークアップにより構造が分かるデータ
- JSON、XML、Avro、Parquetなどが代表例
- ネスト構造（配列やオブジェクトなど）を持つことができ、データの階層が深くなる場合もある
- 一部の属性だけを持つレコードが混在していても問題なく扱える（柔軟性が高い）
- Snowflakeでは、VARIANT、ARRAY、OBJECTというデータ型を使って保存・処理される
  - VARIANTは、他のどんな型（数値、文字列、配列、オブジェクトなど）も含められる万能型
  - OBJECTは、キーと値のペアで構成される構造で、JSONオブジェクトや辞書のようなもの
  - ARRAYは、順序付きの値のリストで、リストや配列に相当する
- Snowflakeでは、これらの型を自由にネストして階層構造を持ったデータを保存できる
- クエリでは、ドット記法やブラケット記法を使って階層構造を辿ってアクセスする
- 例：`data:event.timestamp`のように書くと、eventの中のtimestampにアクセスできる
- Snowflakeは半構造化データを高速かつ効率的に処理できるよう内部的に最適化して保存する
- データのインポート時には、JSONなどをVARIANT型としてそのまま格納することもできる
- 必要に応じて、データの一部を明示的にカラムに分けて保存することも可能
- 半構造化データは構造化データより柔軟性があるため、ログ、センサーデータ、APIレスポンスなどの保存に向いている

#### データクエリ（Querying data）

**構造化データ（Structured data）**
標準的なSQLクエリでアクセス可能

**半構造化データ（Semi-structured data）**
ドット記法やブラケット記法を使用してアクセス

#### コマンド（Commands）
- **COPY INTO**: バッチでのデータ読み込み
- **INSERT**: 少量データの挿入
- **LIST**: ステージ内ファイルの一覧表示

### 3.2 仮想ウェアハウスの説明

#### 標準ウェアハウスとマルチクラスタウェアハウスの比較（Standard warehouses compared to multi-clustered warehouses）

**標準ウェアハウス**
単一クラスタでの処理を行う

**マルチクラスタウェアハウス**
並行処理能力の向上を図るため複数クラスタを使用

#### ウェアハウスサイジング（Warehouse sizing）
X-SmallからXX-Largeまでのサイズ選択が可能で、処理能力とコストのトレードオフを考慮

#### ウェアハウススケーリング（Warehouse scaling）

**スケールイン/アウト（Scaling in or out）**
クラスタ数の増減による並行処理能力の調整

**スケールアップ/ダウン（Scaling up or down）**
ウェアハウスサイズの変更による処理能力の調整

### 3.3 Snowflakeでのテーブル作成とデータ読み込み

#### Snowsightを使用した特定テーブルの識別（Use Snowsight to identify particular tables）
GUIを通じたテーブル管理機能

#### テーブル定義の表示（View the table definition）
スキーマ情報の確認機能

#### テーブルデータのプレビュー（Preview the table data）
データ内容の事前確認機能

#### データ読み込み手法

**INSERT文を使用したデータ読み込み（Use INSERT statements to load data）**
直接的なデータ挿入方法

**Snowsightを使用したデータ読み込み（Use Snowsight to load data）**
GUI経由でのデータインポート

**COPY INTO <table>文の使用（Use COPY INTO <table> statements）**
ファイルからの一括読み込み
- **ファイル形式オプション（File format options）**: 各種ファイル形式の設定

### 3.4 非構造化データの操作方法

#### ディレクトリテーブル（Directory tables）

**ディレクトリテーブルの概念**
- 非構造化データを取り扱う場合、ディレクトリテーブルは本当に便利で、多くの点でLISTコマンドを使うよりも優れている
- ディレクトリテーブルはステージ上にレイヤー化された暗黙のオブジェクトで（独立したデータベース・オブジェクトではない）、ステージ内のデータファイルに関するファイルレベルのメタデータを格納している
- 内部ステージと外部ステージの両方をサポートしている
- 素晴らしいのはこれらのオブジェクトを使用することで、あたかもテーブルであるかのようにディレクトリの内容を照会できること
- ディレクトリテーブルを作成するプロセスは、ステージの作成時にオプションを有効にするか、作成後にステージを変更するだけ
- ディレクトリテーブルの出力にテーブル関数でアクセスし、名前とファイルの場所を様々なSnowflake関数に簡単に渡して処理することができる

**有効化（Enable）**
`DIRECTORY = (ENABLE = TRUE)`の設定

**SELECT文での使用（Use of SELECT statements）**
ファイルメタデータへのアクセス

**ディレクトリテーブル（Directory Table）の詳細**
- ステージ内のファイル情報（メタデータ）を一覧化した特殊なテーブル
- ファイル名、サイズ、最終更新日、`FILE_URL`などが含まれる
- `DIRECTORY = (ENABLE = TRUE)`をステージ作成時に指定して有効化
- SQLで`SELECT * FROM DIRECTORY(@stage_name);`のように参照
- URL生成やファイルの検索に役立つ（例：最新ファイルだけ取得）

#### Pre-signed URLの使用（Use of Pre-signed URLs）

**FUNCTION引数（FUNCTION argument）**
一時的なファイルアクセスURL生成機能

**URLの種類と使い方**
Snowflakeは3種類のURLを用意している：

*Scoped URL*
- 認可付きの一時的URL（最大24時間）
- `BUILD_SCOPED_FILE_URL()`で生成
- 他アカウントとの共有に安全

*File URL*
- 永続的なファイル位置情報を示す内部用URL
- `BUILD_STAGE_FILE_URL()`か`DIRECTORY()`の`FILE_URL`列で得られる

*Pre-signed URL*
- 一時的な公開HTTP URL
- `GET_PRESIGNED_URL()`で生成、アクセス期限も指定可能

### 3.5 Snowflake Cortex LLM関数の使用方法

#### PARSE_DOCUMENT関数
文書の解析機能

#### TRANSLATE関数
翻訳機能

#### CLASSIFY_TEXT関数
テキスト分類機能

#### COMPLETE関数
テキスト補完機能

---

## 4.0 Data Protection and Data Sharing (10%)

### 4.1 Snowflakeでの継続的データ保護

#### Time Travel
- データの「巻き戻し」ボタンがあると想像してみてください。Time Travelはまさにそれを実現します
- データの過去のバージョンを確認し、以前の時点に復元できます
- Time Travelのマイクロパーティションは、1日あたりの平均ストレージコストに影響します

**保存期間**
- Time Travelは、デフォルトで1日間（Standard Editionの場合）、または最大90日間（Enterprise Editionの場合）データを保存するため、どのくらい過去に遡るかを決めることができます

**機能詳細**
- 定義期間内の任意時点で履歴データにアクセス可能
- 主要用途：データ復元、履歴クエリ、データクローン、データ分析
- 保持期間：
  - Standard: 1日（自動有効）
  - Enterprise: 永続オブジェクト0-90日、一時オブジェクト0-1日
- SQL拡張：AT|BEFORE句（TIMESTAMP/OFFSET/STATEMENT）、UNDROPコマンド
- 重要事項：保持期間終了後はFail-safeに移動し復元不可

**Time Travel機能の活用**
- SnowflakeのTime Travel機能では、過去の特定時点のテーブルやスキーマ、データベースをクローン（複製）できる
- クローンする前に、過去の特定日付のデータを`SELECT ... AT (TIMESTAMP => '日時')`で確認できる
- クローンは`CREATE TABLE new_table CLONE old_table AT (TIMESTAMP => '日時')`のように作成する

#### Fail-safe（フェイルセーフ）
- フェイルセーフは、タイムトラベルが利用できなくなった場合の「緊急バックアップ」としてご利用いただけます
- 失われたデータの復旧に7日間の猶予が与えられますが、Snowflakeサポートにサポートを依頼する必要があります
- フェイルセーフのマイクロパーティションも、1日あたりの平均ストレージコストに含まれます

#### ゼロコピークローニング（Cloning）
- この機能を使用すると、追加のストレージをすぐに使用することなく、データの正確なコピーを瞬時に作成できます
- Snowflakeのゼロコピークローニング機能は、任意のテーブル、スキーマ、またはデータベースの「スナップショット」を迅速に取得し、そのオブジェクトの派生コピーを作成する便利な方法を提供します
- この派生コピーは、最初は基盤となるストレージを共有します
- これは、クローンオブジェクトに変更が加えられるまで追加コストが発生しない即時バックアップを作成するのに非常に便利です
- 全ての顧客DB内のすべてのマイクロパーティションファイルをメタデータレイヤーが管理
- クローンが作成されるとマイクロパーティションが共有される
- オリジナルまたはクローンに変更を加えるまで、追加のストレージコストは不要

#### データベースのレプリケーション
- データベースレプリケーションは、アカウント間でデータベースをコピーすることでデータを保護します
- 通常、これらのアカウントは、同じクラウドプロバイダー内の異なるリージョン、または異なるクラウドプロバイダーに配置されます
- これにより、あるプロバイダー/リージョンでシステム障害が発生しても、別のプロバイダー/リージョンでデータを利用できるため、ビジネスの継続性が確保されます

**主な洞察:**
- クロスリージョンレプリケーション: データを異なるリージョンにレプリケートできるため、1つのリージョンで障害が発生した場合でも、データには引き続きアクセスできます
- クラウド間レプリケーション：データを異なるクラウドプロバイダーにレプリケートできるため、1つのプロバイダーが停止の影響を受けても、データに引き続きアクセスできます
- フェイルオーバーとフェイルバック: プライマリデータベースに障害が発生した場合、セカンダリレプリカに切り替えて、操作をスムーズに実行し続けることができます
- 定期的な同期: フェイルオーバー中に不整合が発生しないように、レプリカが同期されていることを確認します

### 4.2 Snowflakeデータ共有機能の定義

#### Snowflake Marketplace

**検索リスティング（Search listings）**
公開データセットの検索機能を提供

**Marketplaceの特徴:**
- パブリック市場、商用データ、標準化UI/UX
- 天気・金融・業界データ、Native Apps提供
- ワンクリック共有、使用量ベース課金

#### Data Exchange

**プライベートデータ共有の使用（Use a private data share）**
招待制でのデータ共有機能

**Data Exchangeの特徴:**
- プライベート招待制、内部・パートナーデータ
- カスタマイズ可能、高度セキュリティ制御
- 組織独自Exchange作成、カスタムポリシー

**共通点:**
ゼロコピーでリアルタイムデータ共有

---

## 詳細解説とベストプラクティス

### Snowflakeによるデータ取り込みのベストプラクティス（パート1）要約

#### 概要
企業のデータ量が急増する中、Snowflakeは多様なデータ（構造化・半構造化・非構造化）の取り込みに対応し、効率的なデータ活用を実現しています。主なデータ取り込み方法として「COPY INTO」「Snowpipe」などがあり、それぞれの特徴とベストプラクティスが紹介されています。

#### 主なデータ取り込み方法

**INSERT**
少量データ向き。大量データには不向き。

**COPY INTO**
バッチ処理向き。仮想ウェアハウスを使い、ファイル単位でトランザクションを管理。並列処理が可能で高速。

**Snowpipe**
継続的なデータ取り込み向き。サーバーレスで自動スケーリング。ファイル到着をトリガーに自動で取り込み。管理コストが低く、レイテンシも低い。

#### COPY INTOとSnowpipeの違い
- COPY INTOはバッチ処理で手動実行、Snowpipeはストリーミング的に自動実行
- COPY INTOはウェアハウス管理が必要、Snowpipeは不要
- COPY INTOはファイル単位でトランザクション管理、Snowpipeはマイクロバッチ単位でコミットされるため厳密なトランザクション保証はない

#### ファイルサイズとコスト

**推奨ファイルサイズ**
10MB以上、最適は100～250MB。5GB以下推奨（大きすぎるとエラー時のリカバリが大変）。

**コスト**
Snowpipeはファイル単位の固定コスト＋処理コストがかかるため、小さいファイルが多い場合はコスト効率が悪化。COPY INTOは大きなファイルや大量処理に向く。

#### ファイル形式・圧縮
- CSV、JSON、Parquet、Avro、ORC、XML（プレビュー）などに対応
- 圧縮ファイル（gzipなど）の利用推奨。ネットワーク効率も向上

#### モニタリング
- COPY_HISTORYビューやAPIで取り込み状況を監視可能
- Snowpipeは非同期処理のため、エラー通知やAPIで状態確認が必要

#### 10のベストプラクティス（抜粋）
1. 継続的な取り込みにはSnowpipe自動取り込みを推奨
2. 初期ロードもSnowpipe自動取り込み＋COPYの併用を検討
3. ファイルサイズは10MB以上、できれば100～250MB
4. フィールドサイズは16MB以下
5. 可能な限りファイル分割や変換を避け、ネイティブ機能を活用
6. サンプルデータで事前にパフォーマンス測定を推奨
7. COPY_HISTORYやエラー通知で取り込み状況を監視
8. Snowpipeはファイル順序保証なし。順序が必要ならCOPYを利用
9. COPY時はパスパーティショニングを活用し効率化
10. クラウドイベントフィルタリングで不要な通知やコストを削減

#### 比較表：COPY INTO vs Snowpipe

| 項目 | COPY INTO | Snowpipe（自動取り込み含む） |
|------|-----------|------------------------------|
| 主な用途 | バッチ処理、大量データの初期ロード | 継続的なデータ取り込み、リアルタイム性が必要な場合 |
| 実行タイミング | 手動またはスケジュール実行 | ファイル到着時に自動実行 |
| ウェアハウス管理 | 必要（サイズ・起動・停止の管理が必要） | 不要（サーバーレス、Snowflakeが自動管理） |
| コスト構造 | ウェアハウスの利用分だけ課金 | ファイルごとに固定料金＋処理コスト |
| ファイルサイズの推奨 | 大きなファイル、大量ファイル向き（10MB～数GB） | 10MB以上推奨（100MB～250MBが最適） |
| ファイル数 | 多数のファイルをまとめて処理可能 | 頻繁にファイルが到着する場合に最適 |
| トランザクション粒度 | ファイル単位で厳密なトランザクション管理 | マイクロバッチ単位でコミット（厳密な保証なし） |
| エラー時の挙動 | エラー時の制御が柔軟（ON_ERROR設定） | エラー通知はあるが、細かな制御は難しい |
| 順序保証 | ファイル順序を制御可能 | 順序保証なし（非同期で並列処理される） |
| モニタリング | 実行時にステータスが即時取得可能 | 非同期処理のためAPIやエラー通知で確認 |
| 初期ロード（大量データ） | ◎（推奨） | 〇（ただし通知設定やバケットリストなど工夫必要） |
| 増分・リアルタイム取込 | △（バッチ処理のみ） | ◎（リアルタイム性・自動化に強い） |
| 設定の手軽さ | シンプルだがウェアハウスやスケジューラの管理必要 | イベント通知や権限設定が必要 |

#### 使い分けのポイント（簡単な目安）
- **初期ロードや大量のバッチ処理** → COPY INTO
- **ファイルが頻繁に届く場合やリアルタイム性重視** → Snowpipe
- **ウェアハウス管理をしたくない、運用を自動化したい** → Snowpipe
- **順序やエラー制御を細かくしたい** → COPY INTO

### Resource Monitors

#### リソースモニターの概念
- 仮想ウェアハウスが消費するクレジット数を監視することで、Snowflakeでのクレジット使用量を管理および制御するためのツール
- クレジットクォータ-: 特定の期間（日次、週次、月次）に許可するクレジットの総数。クォータに達した場合は、管理者への通知や倉庫業務の一時停止などの措置を講じることができます
- リソースモニターは、1つのwarehouseまたは複数のwarehouseに割り当てることができますが、複数のwarehouseがモニターを共有する場合、それらの合計使用量は同じクォータにカウントされることに注意することが重要です

#### 閾値とアクション
- 閾値を到達すると、通知、停止、すぐ停止のアクションが取れる
  - 通知と停止: ウェアハウスによって実行されているすべてのステートメントが完了したら、通知を送信し、割り当てられたすべてのウェアハウスを一時停止します
  - すぐに通知して停止する: 通知を送信し、割り当てられたすべてのウェアハウスを直ちに停止します。これにより、その時点でウェアハウスによって実行されているすべてのステートメントがキャンセルされます
  - 通知: 倉庫に対してアクションは実行しませんが、通知を送信します
- 閾値を90%に設定することで過剰な使用を抑える
- ユーザーかAdministratorに通知できる
- 通知はemailかSnowsightへ

#### 権限と設定
- リソースモニターを作成できるのはACCOUNTADMINロールを持つユーザーのみですが、アカウント管理者は他のロールに権限を付与して、他のユーザーがリソースモニターを表示および変更できるようにすることができます

#### 計算方法
- 例えば、リソースモニターの制限が1000クレジットに設定されている場合、指定された期間または日付範囲内でウェアハウスが700クレジットを消費し、クラウドサービスが300クレジットを消費すると、アラートがトリガーされます
- リソースモニターの制限は、クラウドサービスに対する1日10%の調整を考慮していません。Snowflakeは、クラウドサービスレイヤーによるすべてのクレジット消費量に基づいて、たとえその消費量が課金対象とならなかったとしても、制限に達したかどうかを計算します

#### モニターの種類
- **アカウントモニター**: アカウント内のすべての倉庫のクレジット使用状況を監視します。アカウントモニターは1つのアカウントにつき1つだけ設定できます
- **warehouseモニター**: リソースモニターに割り当てられた倉庫のクレジット使用状況を監視します。アカウントには複数の倉庫モニターを設定できます
- warehouseモニターには1つ以上の倉庫を割り当てることができますが、各倉庫は1つのリソースモニターにのみ割り当てることができます

#### コンピューティングコスト
- 仮想ウェアハウス（ユーザー管理のコンピューティングリソース）
- Snowflakeが管理するコンピューティングリソースを使用する自動クラスタリングやSnowpipeなどのサーバーレス機能
- Snowflakeアーキテクチャのクラウドサービス層：
  - クラウドサービスレイヤーはクレジットを消費しますが、そのクレジットすべてが実際に請求されるわけではありません
  - クラウドサービスの使用量は、クラウドサービスの1日あたりの使用量が仮想ウェアハウスの1日あたりの使用量の10%を超えた場合にのみ課金されます

### Snowflakeエコシステム

#### Snowflake Tools
- Snowflakeは、開発ワークフローを簡素化するコマンドラインインターフェースであるSnowCLI、SnowSQLなどのツールを提供します
- Snowparkを使用すると、開発者はSnowflakeの強力な仮想ウェアハウスエンジンを活用して、Snowflake内で直接Python、Java、またはScalaを使用してスケーラブルなデータアプリケーションを構築できます
- SnowCLIは、SQLクエリの実行、データの管理、日次レポートやバックアップなどのタスクの自動化を行うSnowflakeのコマンドラインツールです
- Snowparkを使用すると、Snowflake内で直接、Python、Java、Scalaなどの言語でより高度なプログラミングが可能になります

#### Snowpark Container Services
コンテナ化された環境でカスタムアプリケーションとモデルをSnowflake内で直接デプロイできます

#### Snowpark Migration Accelerator (SMA) and SnowConvert
コードの変換を自動化することで、レガシープラットフォームからSnowflakeへの移行を簡素化し、作業量と時間を削減します

#### Third-Party Tools
- Snowflakeはトップティアのサードパーティツールと統合し、データライフサイクルのすべての段階を最適化します
- データの取り込みと変換では、Fivetran、Matillion、Airbyteなどのツールがデータの移動と準備をシームレスにします
- dbtはデータモデリングの人気の選択肢であり続けており、Tableau、Looker、Power BIなどのツールは分析とレポートをサポートします
- データ統合：Matillion、Fivetran、Informaticaなどのツールを使えば、他のシステムからSnowflakeにデータを簡単に取り込むことができます。これらのツールが面倒な作業をすべて処理してくれるので、手動でデータを移動する必要はありません
- ビジネスインテリジェンス: Tableau、Looker、Power BIなどのツールを使用すると、生のデータを視覚的なダッシュボードやレポートに変換し、意思決定に役立てることができます
- 機械学習: DataRobotやAmazon SageMakerなどのツールはSnowflakeに直接接続し、データに基づいて将来の傾向を予測できるMLモデルを構築できます

#### Partner Ecosystem
- Snowflake Data Cloud MarketplaceとSnowflakeの広範なパートナーネットワークにより、安全なコラボレーション、データ共有、イノベーションが可能になります
- パートナーは、データガバナンス、セキュリティ、高度な分析、MLのソリューションを提供します
- StreamlitとSnowpark機能と組み合わせて、Snowflakeはモダンなデータ駆動型アプリケーションを構築およびデプロイするための包括的な環境を提供します

#### Industry Standard Drivers
- Open Database Connectivity (ODBC)やJava Database Connectivity (JDBC)などの業界標準ドライバーにより、準拠したツールはSnowflakeに直接接続できます
- Snowflakeは、データセキュリティとガバナンスのためのソリューションを提供するために企業と提携しています。ソフトウェア、ドライバー、インターフェースなどのこれらのツールは、特に規制の厳しい業界で事業を展開している場合、データの保護とコンプライアンス基準の遵守を確実にするのに役立ちます
- 企業は、Snowflakeを最大限に活用するためのコンサルティングおよび実装サービスも提供しています。データの移行やカスタムソリューションの設定など、お客様のビジネスに最適な環境の構築をお手伝いします

### Accounts and Assurances

#### 基本概念
- **Role**: 権限を付与できるエンティティ。ロールはユーザーに割り当てられます。ロールは他のロールにも割り当てることができ、ロール階層を作成します
- **Privilege**: オブジェクトへの定義されたアクセスレベル。複数の異なる権限を使用して、付与されるアクセスの粒度を制御できます
- **User**: 人またはプログラムに関連付けられているかどうかに関係なく、Snowflakeによって認識されるユーザーID

### Backup and Recovery

#### Time Travel詳細
- Time Travelは、デフォルトで1日間（Standard Editionの場合）、または最大90日間（Enterprise Editionの場合）データを保存するため、どのくらい過去に遡るかを決めることができます
- bulk load dataとは、大量のデータを一括でSnowflakeのテーブルに取り込むこと
- COPY INTOコマンドは、外部ストレージ（S3、Azure Blob、ローカルファイルなど）からSnowflakeにデータをロードする際に使う
- COPY INTOはまた、Snowflakeのテーブルから外部ストレージへデータを書き出す（アンロード）ことにも使える

#### FLATTEN table function
- 配列やオブジェクトなどのネスト構造を展開して、リレーショナル形式に変換する関数
- FLATTENを使うと、配列の中の各要素を1行ずつに分解して取り出すことができる
- FLATTENを実行すると、VALUE列（展開された中身）、INDEX列（配列内の位置）、DEPTH列（階層の深さ）などが得られる
- DEPTH列は、ネストされた構造のどの階層に要素があるかを示す指標であり、入れ子が深いほど値が大きくなる

### Snowflakeデータ共有まとめ

#### Secure Data Sharingとは
- Snowflakeのデータ共有機能全体の名前
- データをコピーせずに他のアカウントと共有
- 共有データは読み取り専用
- ストレージ料金は提供者負担、計算料金は利用者負担

#### 基本構造
- **共有（Share）** = データ共有の基本オブジェクト
- **提供方法** = 作った共有をどう相手に渡すか

#### 3つの提供方法

**1. 直接共有**
- 特定のアカウントに個別に共有を提供
- 1対1または1対少数
- 同一リージョン内のみ
- 最もシンプル

**2. リスト**
- 共有を商品化してカタログ形式で提供
- メタデータ追加可能
- 使用状況メトリック取得可能
- 公開先：Snowflake Marketplace（公開）またはプライベート（非公開）

**3. データ交換**
- アカウントグループを作成して管理
- グループ内でのデータ共有
- より高度な管理・制御機能

#### 使い分け
- **少数の特定相手** → 直接共有
- **多数への商品提供** → リスト
- **グループ管理が必要** → データ交換

#### その他
- **リーダーアカウント** = Snowflakeアカウントを持たない相手用の特別アカウント
- 直接共有からリストへの変換も可能

### Snowflakeクラスタリングまとめ

#### クラスタリングとは
- 関連するデータを同じマイクロパーティション内にまとめて配置する仕組み
- 大きなテーブルでクエリ性能を向上させるための機能
- データをコピーせず、物理的な配置を最適化

#### クラスタリングキー
- どの列でデータをまとめるかを指定するキー
- 1つまたは複数の列・式で構成
- 最大3-4列を推奨（それ以上はコスト増）

#### 効果・メリット
- **クエリ高速化**: 不要なマイクロパーティションをスキップ
- **圧縮率向上**: 似たデータが集まるため圧縮効率UP
- **スキャン効率向上**: フィルタリング性能が大幅改善

#### 適用すべきテーブル
- **大きなテーブル**（数TB以上のデータ）
- **頻繁にクエリされる**テーブル
- **選択的なクエリ**（全体の一部だけ読む）が多い
- **更新頻度が低い**テーブル
- **DML操作に対するクエリ比率が高い**テーブル

#### クラスタリングキー選択戦略
- **優先順位1**: 選択フィルターで最も使用される列（日付列など）
- **優先順位2**: 結合述語で頻繁に使用される列
- **カーディナリティ**: 適度な個別値数（高すぎず低すぎず）
- **列順序**: 低カーディナリティ → 高カーディナリティ

#### 自動クラスタリング
- **自動メンテナンス**: Snowflakeが状態を監視・管理
- **ノンブロッキング**: 再クラスタリング中でもDML可能
- **最適効率**: 必要な時だけ実行
- **制御可能**: 一時停止・再開・削除が可能

#### コスト
- **計算コスト**: サーバーレスリソースでクレジット消費
- **ストレージコスト**: 再クラスタリング時に一時的に増加
- **Time Travel/Fail-safe**: 元データ保持で長期間ストレージ消費

#### 基本操作
```sql
-- 作成時に定義
CREATE TABLE sales (...) CLUSTER BY (sale_date);

-- 後から追加
ALTER TABLE sales CLUSTER BY (sale_date, region);

-- 一時停止
ALTER TABLE sales SUSPEND RECLUSTER;

-- 再開
ALTER TABLE sales RESUME RECLUSTER;

-- 削除
ALTER TABLE sales DROP CLUSTERING KEY;

-- 状態確認
SHOW TABLES LIKE 'sales';
```

#### 注意点
- **すべてのテーブルに必要ではない**
- **効果とコストを天秤にかける**
- **少数のテーブルから始めて評価**
- **VARCHAR列は最初の5バイトのみ使用**
- **ハイブリッドテーブルでは使用不可**

### 仮テーブルと一時テーブルまとめ

#### 英語名・キーワード
- **仮テーブル** = Temporary Table (`TEMPORARY` / `TEMP`)
- **一時テーブル** = Transient Table (`TRANSIENT`)

#### 仮テーブル (Temporary Table)
- **持続期間**: セッション終了まで
- **可視性**: 作成したセッション内のみ
- **自動削除**: セッション終了で自動的に完全削除
- **権限**: CREATE TABLE権限不要
- **用途**: ETL中間データ、セッション固有データ
- **命名**: 同名の他テーブルを隠す（優先される）
- **Time Travel**: 0-1日（実際はセッション終了まで）
- **Fail-safe**: なし

#### 一時テーブル (Transient Table)
- **持続期間**: 明示的に削除するまで
- **可視性**: 適切な権限を持つ全ユーザー
- **削除**: 手動削除が必要
- **権限**: 通常のCREATE TABLE権限が必要
- **用途**: 再構築可能データ、ログデータ、キャッシュ
- **Time Travel**: 0-1日
- **Fail-safe**: なし

#### 永続テーブル (比較用)
- **持続期間**: 明示的に削除するまで
- **可視性**: 適切な権限を持つ全ユーザー
- **Time Travel**: 0-90日（Edition依存）
- **Fail-safe**: 7日

#### ストレージコスト
- **仮テーブル**: Time Travelのみ（セッション中）
- **一時テーブル**: Time Travelのみ（Fail-safeなし）
- **永続テーブル**: Time Travel + Fail-safe

#### 作成構文
```sql
-- 仮テーブル
CREATE TEMPORARY TABLE temp_name (...);
CREATE TEMP TABLE temp_name (...);

-- 一時テーブル
CREATE TRANSIENT TABLE trans_name (...);
```

#### 使い分け
- **仮テーブル**: セッション限定の一時処理
- **一時テーブル**: 複数セッション共有の一時データ
- **永続テーブル**: 重要なビジネスデータ

#### 注意点
- **データ回復**: どちらもFail-safe期間なしで完全削除
- **テーブル変換**: 作成後は他タイプに変換不可
- **クローン制限**: 特定の組み合わせのみ可能
- **命名競合**: 仮テーブルが他テーブルより優先

#### 一時データベース・スキーマ
- **一時スキーマ**: `CREATE TRANSIENT SCHEMA`
- **一時データベース**: `CREATE TRANSIENT DATABASE`
- **継承**: 一時スキーマ内のテーブルは自動的に一時テーブル

### Snowflakeデータロード完全まとめ

#### データロードの基本概念

**ステージとは**
- **定義**: データファイルを置く場所（クラウドストレージ）
- **役割**: データをテーブルに入れる前の中継地点
- **目的**: データ移動の準備場所

**ステージの種類**

*外部ステージ*
- **場所**: 自分のクラウドストレージ（S3、GCS、Azure）
- **メリット**: 既存ストレージを活用
- **注意**: 異なる地域だと転送料金発生

*内部ステージ（Snowflake管理）*
- **ユーザーステージ**: 各ユーザー専用の場所
- **テーブルステージ**: 各テーブル専用、テーブル所有者のみ操作可能
- **名前付きステージ**: 権限付与で他ユーザーと共有可能

#### データロード方法

**1. 一括ロード（COPYコマンド）**
- **用途**: 大量データを一度にロード
- **特徴**: ユーザー指定の仮想ウェアハウス使用
- **適用**: バッチ処理、初期データ投入、日次処理
- **変換**: ロード時にデータ変換可能

**2. 連続ロード（Snowpipe）**
- **用途**: 小さなデータを継続的にロード
- **特徴**: Snowflake自動リソース管理、サーバーレス
- **遅延**: ファイル追加から数分でロード
- **適用**: ログファイル、準リアルタイム処理

**3. ストリーミングロード（Snowpipe Streaming）**
- **用途**: リアルタイムデータ投入
- **特徴**: ファイル不要、直接テーブル書き込み
- **遅延**: 超低遅延
- **適用**: IoTデータ、金融取引、Kafka連携

#### データロードの流れ
1. **データファイル準備**: CSV、JSONファイル作成
2. **ステージにアップロード**: PUTコマンドまたはクラウドツール
3. **テーブルにロード**: COPYコマンド実行

#### 外部テーブル vs ステージ

**ステージ（通常のロード）**
- **目的**: データ移動の準備場所
- **処理**: ファイル内容をSnowflakeテーブルにコピー
- **例**: 引っ越し業者のトラック（荷物を運ぶ）
- **結果**: データがSnowflakeに保存される

**外部テーブル**
- **目的**: ファイルそのものをテーブルとして扱う
- **処理**: ファイルを直接読み取り（移動なし）
- **例**: リモートアクセス（遠隔地のファイルを直接操作）
- **結果**: 元ファイルを直接参照
- **注意**: 外部テーブルでもステージは使用（場所の定義）

#### ステージの使い分け

**テーブルステージ**
- **特徴**: 1テーブル専用、テーブル所有者のみ操作可能
- **用途**: 単一テーブル専用、責任者が1人、シンプル構成
- **権限**: テーブル所有者のみPUT/COPY、他ユーザーはSELECTのみ
- **本番使用**: 可能（個人用に限らない）

**名前付きステージ**
- **特徴**: 複数テーブル対応、権限管理可能、再利用可能
- **用途**: チーム作業、複数テーブルへのロード、部門間連携
- **権限**: USAGE（参照）、READ（読み取り）、WRITE（書き込み）
- **共有範囲**: 同一Snowflakeアカウント内のみ

#### 便利機能

**スキーマ検出**
- **機能**: ファイルから自動でテーブル構造推測
- **対応形式**: CSV、JSON、Parquet、Avro、ORC
- **関数**: `INFER_SCHEMA`、`GENERATE_COLUMN_DESCRIPTION`

**データ変換**
- **機能**: ロード時にデータ加工
- **内容**: 列並び替え、省略、型変換、切り捨て

#### 使い分け指針

**データの取り扱い方法**
- **データ取り込み**: ステージ + COPY
- **データ参照のみ**: 外部テーブル

**ロード方法の選択**
- **大量・定期**: 一括ロード（COPY）
- **小量・継続**: Snowpipe
- **リアルタイム**: Snowpipe Streaming

**ステージの選択**
- **1テーブル1責任者**: テーブルステージ
- **複数テーブル・チーム作業**: 名前付きステージ

**コスト考慮**
- **ストレージ節約**: 外部テーブル
- **パフォーマンス重視**: 通常のテーブル（ロード）

#### 実用例

**基本的なロード**
```sql
-- 1. ステージ作成
CREATE STAGE my_stage;

-- 2. ファイルアップロード
PUT file://data.csv @my_stage;

-- 3. テーブルにロード
COPY INTO my_table FROM @my_stage/data.csv;
```

**外部テーブル**
```sql
-- 1. ステージ作成
CREATE STAGE ext_stage URL='s3://my-bucket/';

-- 2. 外部テーブル作成
CREATE EXTERNAL TABLE ext_table (col1 STRING)
LOCATION=@ext_stage/ FILE_FORMAT=(TYPE=CSV);

-- 3. 直接クエリ
SELECT * FROM ext_table;
```

**要点**: データの量・頻度・リアルタイム性・チーム構成に応じて最適な方法を選択する！

### Snowflake非構造化データ完全まとめ

#### 非構造化データとは
- **定義**: 事前定義されたスキーマに適合しない情報
- **例**: 画像、動画、音声、PDF、Word文書、業界固有ファイル
- **特徴**: 通常のテーブル構造では扱えないデータ

#### Snowflakeでできること
- **安全なアクセス**: クラウドストレージ内のファイルに安全にアクセス
- **URL共有**: ファイルアクセス用URLを他の人と共有
- **メタデータ管理**: ファイル情報をSnowflakeテーブルで管理
- **ファイル処理**: 非構造化データを処理・分析

#### ファイルアクセス用URL（3種類）

**1. スコープURL**
- **用途**: 一時的な安全なアクセス
- **特徴**:
  - 24時間で期限切れ
  - ビュー権限で制御
  - アクセス履歴が記録される
- **適用**: 社内での限定的なファイル共有
- **関数**: `BUILD_SCOPED_FILE_URL`

**2. ファイルURL**
- **用途**: 永続的なファイル参照
- **特徴**:
  - 期限なし
  - ステージ権限で制御
  - REST API経由でアクセス
- **適用**: カスタムアプリケーション開発
- **関数**: `BUILD_STAGE_FILE_URL`

**3. 事前署名付きURL**
- **用途**: 簡単なファイル共有
- **特徴**:
  - 認証不要でアクセス可能
  - 有効期限を設定可能
  - ブラウザで直接開ける
- **適用**: BIツール、レポート、外部共有
- **関数**: `GET_PRESIGNED_URL`

#### ディレクトリテーブル

**基本機能**
- **機能**: ステージ内ファイルのカタログ
- **情報**: ファイル名、サイズ、更新日時、MD5ハッシュ
- **有効化**: `CREATE STAGE ... DIRECTORY = (ENABLE = true)`

**使用頻度**
- **一般的なデータ分析**: **ほとんど使わない**（5%未満）
- **データエンジニアリング**: たまに使う（20-30%）
- **非構造化データ処理**: よく使う（50%以上）

**実用的な用途**
- **ファイル処理の自動化**: 新しいファイルだけを処理
- **データパイプライン制御**: 未処理ファイルの特定
- **ファイル品質チェック**: 異常サイズ・命名規則違反の検出
- **データガバナンス**: 重複・古いファイルの管理

#### 暗号化オプション

**クライアント側暗号化（デフォルト）**
- **特徴**: Snowflake管理の暗号化キー
- **制限**: 外部ツールから読み取り不可

**サーバー側暗号化**
- **特徴**: 外部ツールからアクセス可能
- **設定**: `ENCRYPTION = (TYPE = 'SNOWFLAKE_SSE')`

#### 非構造化データの処理方法

**1. 外部関数**
- **用途**: 外部サービス（AWS Textract、Google Document AI等）を利用
- **例**: OCR、画像認識、自然言語処理

**2. UDF/ストアドプロシージャ**
- **用途**: JavaやPythonでファイル処理
- **例**: 画像解析、テキスト抽出、機械学習

#### 実用例

**企業での活用例**
- **文書管理**: 契約書、報告書のPDF管理
- **画像分析**: 商品画像、品質検査画像の処理
- **メディア管理**: 動画、音声ファイルのメタデータ管理

**具体的な実装**
```sql
-- 商品画像管理
SELECT
    product_name,
    BUILD_SCOPED_FILE_URL(@product_images, image_file) as image_url
FROM products;

-- 保険証券PDF配信
SELECT
    policy_number,
    GET_PRESIGNED_URL(@insurance_docs, pdf_file, 3600) as download_link
FROM policy_documents;
```

#### 外部テーブル vs ディレクトリテーブル

**外部テーブル**
- **目的**: ファイルの**中身**をテーブルとして直接クエリ
- **対象**: 構造化データ（CSV、JSON、Parquet等）
- **処理**: ファイル内容を行・列として扱う

**ディレクトリテーブル**
- **目的**: ファイルの**メタデータ**を管理
- **対象**: ファイル情報（名前、サイズ、更新日等）
- **処理**: ファイル自体の属性を扱う

**関係性**
- **基本的に別々の仕組み**
- **組み合わせは稀**: それぞれ独立して使用
- **外部テーブルが主役**: データ参照の王道
- **ディレクトリテーブル**: ファイル管理・監視用

#### 使い分け指針

**構造化データ（CSV、JSON等）**
- **外部テーブル**: ファイル内容を分析
- **ディレクトリテーブル**: ほとんど使わない

**非構造化データ（画像、PDF等）**
- **ディレクトリテーブル**: ファイル一覧・URL生成
- **外部テーブル**: 使用不可（中身が構造化されていない）

#### 重要ポイント
- **非構造化データ**: 画像・文書・動画等のファイル管理・共有
- **URL種類**: 用途に応じてスコープ・ファイル・事前署名付きを選択
- **ディレクトリテーブル**: 特殊用途（一般的には使わない）
- **セキュリティ**: 暗号化とアクセス制御の適切な設定

**要するに**: Snowflakeで画像・動画・文書などの非構造化データを安全に管理・共有・処理できる仕組み。普通のデータ分析では使わないが、ファイル管理が必要な業務では重要な機能！

### Snowflake関連の要点まとめ

#### Time Travel
- **機能**: 定義期間内の任意時点で履歴データにアクセス可能
- **主要用途**: データ復元、履歴クエリ、データクローン、データ分析
- **保持期間**:
  - Standard: 1日（自動有効）
  - Enterprise: 永続オブジェクト0-90日、一時オブジェクト0-1日
- **SQL拡張**: AT|BEFORE句（TIMESTAMP/OFFSET/STATEMENT）、UNDROPコマンド
- **重要事項**: 保持期間終了後はFail-safeに移動し復元不可

#### クローニング考慮事項
- **アクセス制御**: 子オブジェクトの権限は継承、コンテナ自体は継承されない
- **オブジェクト特性**:
  - シーケンス/外部キー: 同一コンテナ内は新参照、外部は元参照
  - クラスタリングキー: 継承されるが自動クラスタリングは一時停止
  - ステージ: 外部はクローン可、内部は個別クローン不可
  - パイプ/タスク/アラート: デフォルトで一時停止/中断
- **注意点**:
  - クローニング中のDDL操作は名前競合リスク
  - データ保持期間0の場合はDML変更でエラー可能性
- **Time Travelクローニング**: 指定時点で存在しないオブジェクトはエラー

#### Data Exchange vs Snowflake Marketplace
- **Marketplace**:
  - パブリック市場、商用データ、標準化UI/UX
  - 天気・金融・業界データ、Native Apps提供
  - ワンクリック共有、使用量ベース課金
- **Data Exchange**:
  - プライベート招待制、内部・パートナーデータ
  - カスタマイズ可能、高度セキュリティ制御
  - 組織独自Exchange作成、カスタムポリシー
- **共通点**: ゼロコピーでリアルタイムデータ共有

#### CREATE SHARE
- **作成物**: 空の共有オブジェクト（データ共有のコンテナ）
- **初期状態**: データベース/テーブル未含、アカウント未追加、使用不可
- **完成手順**:
  1. GRANTでデータベース・オブジェクト追加
  2. ALTER SHAREで共有先アカウント追加
- **特徴**: 段階的構築、権限ベース、ゼロコピー、リアルタイム更新

#### ベストプラクティス
- **Time Travel**: 最低1日保持推奨、ストレージコスト考慮
- **クローニング**: 事前データ保持期間確認、大規模時はDDL/DML制限
- **データ共有**: 用途に応じてMarketplace/Exchange選択
- **権限管理**: COPY GRANTS活用、事前権限設計重要